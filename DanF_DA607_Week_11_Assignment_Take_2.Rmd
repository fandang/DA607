---
title: "DA607 Week 11 Assignment"
author: "Dan Fanelli"
output: html_document
---

# Assignment: Document Classification

It can be useful to be able to classify new "test" documents using already classified "training" documents.  A common example is using a corpus of labeled spam and ham (non-spam) e-mails to predict whether or not a new document is spam.  

For this project, you can start with a spam/ham dataset, then predict the class of new documents (either withheld from the training dataset or from another source such as your own spam folder).   One example corpus:  https://spamassassin.apache.org/publiccorpus/

For more adventurous students, you are welcome (encouraged!) to come up with a different set of documents (including scraped web pages!?) that have already been classified (e.g. tagged), then analyze these documents to predict how new documents should be classified.

### 4 Steps from the YouTube video:

https://www.youtube.com/watch?v=ytUHvMNnzZk

* Read in the Data
* Count occurrences of each word (Train)
* Classify each Test Mail (Test)
* Print Results

```{r message=FALSE, warning=FALSE}

MAX_WORDS <- 2000

words_df <- data.frame(WORD=character(), HAM_COUNT=integer(), SPAM_COUNT=integer())

ham_word_count <- 0
ham_files <- list.files(path="hw11/easy_ham/", full.names = TRUE)
spam_word_count <- 0
spam_files <- list.files(path="hw11/spam_2/", full.names = TRUE)

for (ham_file in ham_files){
  if(ham_word_count < MAX_WORDS){
    current_content <- readLines(cur_file)
    str_replace_all(x, "[[:punct:]]", " ")
    file_contents <- c(file_contents, current_content)
    ham_word_count <- (i+1)
  }
}


ham_corpus <- get_corpus()


load_train_files <- function(the_dir, is_spam){
  the_files <- list.files(path=the_dir, full.names = TRUE)
  i <- 0
  for (cur_file in the_files){
    if(i < MAX_FILES){
      current_content <- readLines(cur_file)
      file_contents <- c(file_contents, current_content)
      i <- (i+1)
    }
  }
  the_corpus <- Corpus(VectorSource(file_contents))
  return (the_corpus)
}

```

```{r message=FALSE, warning=FALSE}
# Get the Ham and Spam Corpuses:
ham_corpus <- get_corpus("hw11/easy_ham/")
length(ham_corpus)
head(ham_corpus, n=10)
inspect(ham_corpus[1:10])

spam_corpus <- get_corpus("hw11/spam_2/")
length(spam_corpus)
head(spam_corpus, n=10)
inspect(spam_corpus[1:10])

# general filtering opts:
tdm_dtm_opts <- list(removePunctuation=TRUE, removeNumbers=TRUE, stripWhitespace=TRUE, tolower=TRUE)

# create the TDMs
spam_tdm <- TermDocumentMatrix(spam_corpus,control=tdm_dtm_opts)
spam_tdm

ham_tdm <- TermDocumentMatrix(ham_corpus,control=tdm_dtm_opts)
ham_tdm

# check out the frequent terms:
spam_freq_terms <- findFreqTerms(spam_tdm, TOP_X_TERMS)
spam_freq_terms

ham_freq_terms <- findFreqTerms(ham_tdm, TOP_X_TERMS)
ham_freq_terms

# Create Spam and Ham Data Frames
spam_df <- as.data.frame(as.table(spam_tdm))
spam_df$spam_ham <- "SPAM"
kable(head(spam_df, n = 20))

ham_df <- as.data.frame(as.table(ham_tdm))
ham_df$spam_ham <- "HAM"
kable(head(ham_df, n = 20))

all_df <- rbind(ham_df, spam_df)
head(all_df, n=20)
# model time:
```  
  

